{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from google.cloud.storage import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT=PROJECT_ID[0]\n",
    "BUCKET_NAME = f\"{PROJECT}-tweet-sentiment-analysis\"\n",
    "BUCKET= f\"gs://{PROJECT}-tweet-sentiment-analysis\"\n",
    "ROOT='sentiment-classifcation'\n",
    "MODEL_DIR=os.path.join(ROOT,'models').replace(\"\\\\\",\"/\")\n",
    "PACKAGES_DIR=os.path.join(ROOT,'packages').replace(\"\\\\\",\"/\")\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://rare-result-248415-tweet-sentiment-analysis/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'rare-result-248415-tweet-sentiment-analysis' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l {REGION} {BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: gs://rare-result-248415-tweet-sentiment-analysis/sentiment-classifcation\n"
     ]
    }
   ],
   "source": [
    "!gsutil rm -r {BUCKET}/{ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_mapping={\n",
    "    0:\"negative\",\n",
    "    2:\"neutral\",\n",
    "    4:\"positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data can be downloaded from: https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_twitter = pd.read_csv(BUCKET+\"/Data/sentiment_140/training.csv\",encoding=\"latin1\", header=None)\\\n",
    "             .rename(columns={\n",
    "                 0:\"sentiment\",\n",
    "                 1:\"id\",\n",
    "                 2:\"time\",\n",
    "                 3:\"query\",\n",
    "                 4:\"username\",\n",
    "                 5:\"text\"\n",
    "             })[[\"sentiment\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_twitter[\"sentiment_label\"] = df_twitter[\"sentiment\"].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data processing fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "import re\n",
    "\n",
    "class TextPreprocessor(object):\n",
    "    def _clean_line(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
    "        text = re.sub(r\"#[A-Za-z0-9]+\", \"\", text)\n",
    "        text = text.replace(\"RT\",\"\")\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def __init__(self, vocab_size, max_sequence_length):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "        self._tokenizer = None\n",
    "\n",
    "    def fit(self, text_list):        \n",
    "        # Create vocabulary from input corpus.\n",
    "        text_list_cleaned = [self._clean_line(txt) for txt in text_list]\n",
    "        tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
    "        tokenizer.fit_on_texts(text_list)\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def transform(self, text_list):        \n",
    "        # Transform text to sequence of integers\n",
    "        text_list = [self._clean_line(txt) for txt in text_list]\n",
    "        text_sequence = self._tokenizer.texts_to_sequences(text_list)\n",
    "\n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated\n",
    "        # at the beginning.\n",
    "        padded_text_sequence = sequence.pad_sequences(\n",
    "          text_sequence, maxlen=self._max_sequence_length)\n",
    "        return padded_text_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some small test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 3],\n",
       "       [0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import TextPreprocessor\n",
    "\n",
    "processor = TextPreprocessor(5, 5)\n",
    "processor.fit(['hello machine learning','test'])\n",
    "processor.transform(['hello machine learning',\"lol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSES = {'negative':0, 'positive': 1}  # label-to-int mapping\n",
    "VOCAB_SIZE = 25000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from preprocess import TextPreprocessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sents = df_twitter.text\n",
    "labels = np.array(df_twitter.sentiment_label.map(CLASSES))\n",
    "\n",
    "# Train and test split\n",
    "X, _, y, _ = train_test_split(sents, labels, test_size=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Create vocabulary from training corpus.\n",
    "processor = TextPreprocessor(VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "processor.fit(X_train)\n",
    "\n",
    "# Preprocess the data\n",
    "train_texts_vectorized = processor.transform(X_train)\n",
    "eval_texts_vectorized = processor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./processor_state.pkl', 'wb') as f:\n",
    "    pickle.dump(processor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE=.001\n",
    "EMBEDDING_DIM=50\n",
    "FILTERS=64\n",
    "DROPOUT_RATE=0.5\n",
    "POOL_SIZE=3\n",
    "NUM_EPOCH=1\n",
    "BATCH_SIZE=128\n",
    "KERNEL_SIZES=[2,5,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, filters, kernel_sizes, dropout_rate, pool_size, embedding_matrix):\n",
    "    \n",
    "    # Input layer\n",
    "    model_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    # Embedding layer\n",
    "    z = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        weights=[embedding_matrix]\n",
    "    )(model_input)\n",
    "\n",
    "    z = tf.keras.layers.Dropout(dropout_rate)(z)\n",
    "\n",
    "    # Convolutional block\n",
    "    conv_blocks = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv = tf.keras.layers.Convolution1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\",\n",
    "            bias_initializer='random_uniform',\n",
    "            strides=1)(z)\n",
    "        conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
    "        conv = tf.keras.layers.Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "        \n",
    "    z = tf.keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "    z = tf.keras.layers.Dropout(dropout_rate)(z)\n",
    "    z = tf.keras.layers.Dense(100, activation=\"relu\")(z)\n",
    "    model_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = tf.keras.models.Model(model_input, model_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Pretrained Glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = Client()\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "temp_folder = \"Data/embeddings/\"\n",
    "if not os.path.exists(temp_folder):\n",
    "    os.makedirs(temp_folder)\n",
    "blob = bucket.get_blob(\"Data/embeddings/glove.twitter.27B.50d.txt\")\n",
    "downloaded_file = blob.download_to_filename(temp_folder+'/glove.twitter.27B.50d.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coaefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coaefs(*o.strip().split()) for o in\n",
    "                                                open(temp_folder+\"/glove.twitter.27B.50d.txt\",\"r\",encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "word_index = processor._tokenizer.word_index\n",
    "nb_words = min(VOCAB_SIZE, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= VOCAB_SIZE: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create - compile - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 09:51:17.452998: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-01-27 09:51:17.453048: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-27 09:51:17.453083: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tweet-sentiment-analysis): /proc/driver/nvidia/version does not exist\n",
      "2022-01-27 09:51:17.453521: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-27 09:51:17.592086: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2299995000 Hz\n",
      "2022-01-27 09:51:17.593534: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5598d59647e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-27 09:51:17.593571: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "model = create_model(VOCAB_SIZE, EMBEDDING_DIM, FILTERS, KERNEL_SIZES, DROPOUT_RATE,POOL_SIZE, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile model with learning parameters.\n",
    "optimizer = tf.keras.optimizers.Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8438/8438 - 228s - loss: 0.4833 - acc: 0.7664 - val_loss: 0.4259 - val_acc: 0.8043\n"
     ]
    }
   ],
   "source": [
    "#keras train\n",
    "history = model.fit(\n",
    "    train_texts_vectorized, \n",
    "    y_train, \n",
    "    epochs=NUM_EPOCH, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(eval_texts_vectorized, y_test),\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_acc',\n",
    "            min_delta=0.005,\n",
    "            patience=3,\n",
    "            factor=0.5),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.005, \n",
    "            patience=5, \n",
    "            verbose=0, \n",
    "            mode='auto'\n",
    "        ),\n",
    "        tf.keras.callbacks.History()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"history.pkl\",'wb') as file:\n",
    "    pickle.dump(history.history,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('keras_saved_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Prepare custom model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_prediction.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class CustomModelPrediction(object):\n",
    "\n",
    "  def __init__(self, model, processor):\n",
    "    # Class gets instantiated with a trained model file and a persisted processor\n",
    "    self._model = model\n",
    "    self._processor = processor\n",
    "\n",
    "  def _postprocess(self, predictions):\n",
    "    # Create an output signature\n",
    "    labels = ['negative', 'positive']\n",
    "    return [\n",
    "        {\n",
    "            \"label\":labels[int(np.round(prediction))],\n",
    "            \"score\":float(np.round(prediction,4))\n",
    "        } for prediction in predictions]\n",
    "\n",
    "  def predict(self, instances, **kwargs):\n",
    "    # Clean the data, make predictions and postprocess\n",
    "    preprocessed_data = self._processor.transform(instances)\n",
    "    predictions =  self._model.predict(preprocessed_data)\n",
    "    labels = self._postprocess(predictions)\n",
    "    return labels\n",
    "\n",
    "  @classmethod\n",
    "  def from_path(cls, model_dir):\n",
    "    # Load the keras model and the persisted processor\n",
    "    import tensorflow.keras as keras\n",
    "    \n",
    "    model = keras.models.load_model(\n",
    "      os.path.join(model_dir,'saved_model.pb'))\n",
    "    \n",
    "    # I know, pickle is bad and I should feel bad\n",
    "    with open(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
    "      processor = pickle.load(f)\n",
    "\n",
    "    return cls(model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests = ([\"God I hate the north\",\"god I love this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.06350000202655792},\n",
       " {'label': 'positive', 'score': 0.9175999760627747}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_prediction import CustomModelPrediction\n",
    "\n",
    "classifier = CustomModelPrediction.from_path('.')\n",
    "results = classifier.predict(requests)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Package it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "  name=\"tweet_sentiment_classifier\",\n",
    "  version=\"0.1\",\n",
    "  include_package_data=True,\n",
    "  scripts=[\"preprocess.py\", \"model_prediction.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap it up and copy to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing tweet_sentiment_classifier.egg-info/PKG-INFO\n",
      "writing dependency_links to tweet_sentiment_classifier.egg-info/dependency_links.txt\n",
      "writing top-level names to tweet_sentiment_classifier.egg-info/top_level.txt\n",
      "reading manifest file 'tweet_sentiment_classifier.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tweet_sentiment_classifier.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating tweet_sentiment_classifier-0.1\n",
      "creating tweet_sentiment_classifier-0.1/tweet_sentiment_classifier.egg-info\n",
      "creating tweet_sentiment_classifier-0.1/tweet_sentiment_classifier.egg-info/.ipynb_checkpoints\n",
      "copying files to tweet_sentiment_classifier-0.1...\n",
      "copying model_prediction.py -> tweet_sentiment_classifier-0.1\n",
      "copying preprocess.py -> tweet_sentiment_classifier-0.1\n",
      "copying setup.py -> tweet_sentiment_classifier-0.1\n",
      "copying tweet_sentiment_classifier.egg-info/PKG-INFO -> tweet_sentiment_classifier-0.1/tweet_sentiment_classifier.egg-info\n",
      "copying tweet_sentiment_classifier.egg-info/SOURCES.txt -> tweet_sentiment_classifier-0.1/tweet_sentiment_classifier.egg-info\n",
      "copying tweet_sentiment_classifier.egg-info/dependency_links.txt -> tweet_sentiment_classifier-0.1/tweet_sentiment_classifier.egg-info\n",
      "copying tweet_sentiment_classifier.egg-info/top_level.txt -> tweet_sentiment_classifier-0.1/tweet_sentiment_classifier.egg-info\n",
      "copying tweet_sentiment_classifier.egg-info/.ipynb_checkpoints/top_level-checkpoint.txt -> tweet_sentiment_classifier-0.1/tweet_sentiment_classifier.egg-info/.ipynb_checkpoints\n",
      "Writing tweet_sentiment_classifier-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'tweet_sentiment_classifier-0.1' (and everything under it)\n",
      "Copying file://./dist/tweet_sentiment_classifier-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.8 KiB/  1.8 KiB]                                                \n",
      "Operation completed over 1 objects/1.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!python setup.py sdist\n",
    "!gsutil cp ./dist/tweet_sentiment_classifier-0.1.tar.gz {BUCKET}/{PACKAGES_DIR}/tweet_sentiment_classifier-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://keras_saved_model.h5 [Content-Type=application/x-hdf5]...\n",
      "/ [1 files][ 19.9 MiB/ 19.9 MiB]                                                \n",
      "Operation completed over 1 objects/19.9 MiB.                                     \n",
      "Copying file://processor_state.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 27.2 MiB/ 27.2 MiB]                                                \n",
      "Operation completed over 1 objects/27.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp keras_saved_model.h5 {BUCKET}/{MODEL_DIR}/\n",
    "!gsutil cp processor_state.pkl {BUCKET}/{MODEL_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create model and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME='tweet_sentiment_classifier'\n",
    "VERSION_NAME='V1'\n",
    "RUNTIME_VERSION='2.3' # tensorflow version\n",
    "REGION='europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "\u001B[1;31mERROR:\u001B[0m (gcloud.ai-platform.models.create) Resource in projects [rare-result-248415] is the subject of a conflict: Field: model.name Error: A model with the same name already exists.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: A model with the same name already exists.\n",
      "    field: model.name\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create {MODEL_NAME} --regions {REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME                        DEFAULT_VERSION_NAME\n",
      "tweet_sentiment_classifier\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models list --region global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Deleting version [V1]......done.                                               \n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions delete {VERSION_NAME} --model {MODEL_NAME} --region global --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create {VERSION_NAME} \\\n",
    "--model {MODEL_NAME} \\\n",
    "--origin {BUCKET}/{MODEL_DIR} \\\n",
    "--python-version 3.7 \\\n",
    "--runtime-version {RUNTIME_VERSION} \\\n",
    "--package-uris {BUCKET}/{PACKAGES_DIR}/tweet_sentiment_classifier-0.1.tar.gz \\\n",
    "--prediction-class=model_prediction.CustomModelPrediction \\\n",
    "--region global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests = [\n",
    "    \"god this episode sucks\",\n",
    "    \"meh, I kinda like it\",\n",
    "    \"what were the writer thinking, omg!\",\n",
    "    \"omg! what a twist, who would'v though :o!\"\n",
    "    \"woohoow, sansa for the win!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JSON format the requests\n",
    "request_data = {'instances': requests}\n",
    "\n",
    "# Authenticate and call CMLE prediction API \n",
    "credentials = GoogleCredentials.get_application_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 4.13 ms, total: 19.8 ms\n",
      "Wall time: 212 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "api = discovery.build('ml', 'v1')\n",
    "\n",
    "parent = 'projects/{}/models/{}/versions/{}'.format(PROJECT, MODEL_NAME, VERSION_NAME)\n",
    "parent = 'projects/{}/models/{}'.format(PROJECT, MODEL_NAME)\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.023399999365210533},\n",
       " {'label': 'positive', 'score': 0.6355000138282776},\n",
       " {'label': 'positive', 'score': 0.6098999977111816},\n",
       " {'label': 'positive', 'score': 0.5871000289916992}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}